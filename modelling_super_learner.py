import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix, make_scorer
from sklearn.ensemble import StackingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import lightgbm as lgb
import joblib
import warnings
import json
import os
import time
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

warnings.filterwarnings('ignore')

def specificity_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—ç‰¹å¼‚æ€§ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        cm = confusion_matrix(y_true, y_pred)
        specificities = []
        for i in range(len(cm)):
            tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
            fp = np.sum(cm[:, i]) - cm[i, i]
            spec = tn / (tn + fp) if (tn + fp) > 0 else 0
            specificities.append(spec)
        return np.mean(specificities)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tn / (tn + fp) if (tn + fp) > 0 else 0

def npv_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—é˜´æ€§é¢„æµ‹å€¼ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        cm = confusion_matrix(y_true, y_pred)
        npvs = []
        for i in range(len(cm)):
            tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
            fn = np.sum(cm[i, :]) - cm[i, i]
            npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            npvs.append(npv)
        return np.mean(npvs)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tn / (tn + fn) if (tn + fn) > 0 else 0

def ppv_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—é˜³æ€§é¢„æµ‹å€¼ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        cm = confusion_matrix(y_true, y_pred)
        ppvs = []
        for i in range(len(cm)):
            tp = cm[i, i]
            fp = np.sum(cm[:, i]) - tp
            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
            ppvs.append(ppv)
        return np.mean(ppvs)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tp / (tp + fp) if (tp + fp) > 0 else 0

def load_saved_models(model_paths_file):
    """åŠ è½½æ‰€æœ‰ä¿å­˜çš„æœ€ä¼˜æ¨¡å‹"""
    print("=== åŠ è½½ä¿å­˜çš„æœ€ä¼˜æ¨¡å‹ ===")
    
    if not os.path.exists(model_paths_file):
        raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„æ–‡ä»¶ä¸å­˜åœ¨: {model_paths_file}")
    
    with open(model_paths_file, 'r') as f:
        paths_info = json.load(f)
    
    output_dir = paths_info['output_dir']
    model_paths = paths_info['model_paths']
    n_classes = paths_info['n_classes']
    is_multiclass = paths_info['is_multiclass']
    scaler_path = paths_info.get('scaler_path')
    
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"æ¨¡å‹æ•°é‡: {len(model_paths)}")
    print(f"æ˜¯å¦ä¸ºå¤šåˆ†ç±»: {is_multiclass}")
    
    loaded_models = {}
    
    for model_name, model_path in model_paths.items():
        print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {model_name}")
        print(f"è·¯å¾„: {model_path}")
        
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {model_path}")
            continue
        
        try:
            model = joblib.load(model_path)
            loaded_models[model_name] = model
            print(f"âœ… æˆåŠŸåŠ è½½: {model_name} - {type(model).__name__}")
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {model_name} - {str(e)}")
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(loaded_models)} ä¸ªæ¨¡å‹")
    return loaded_models, output_dir, n_classes, is_multiclass, scaler_path, paths_info

def load_data_for_super_learner(paths_info):
    """åŠ è½½åŸå§‹æ•°æ®ç”¨äºSuper Learnerè®­ç»ƒï¼Œé¿å…æ•°æ®æ³„éœ²"""
    print("\n=== åŠ è½½åŸå§‹æ•°æ® ===")
    
    original_data_path = paths_info['original_data_path']
    scaler_path = paths_info['scaler_path']
    is_multiclass = paths_info['is_multiclass']
    
    print(f"åŸå§‹æ•°æ®è·¯å¾„: {original_data_path}")
    print(f"Scalerè·¯å¾„: {scaler_path}")
    
    if not os.path.exists(original_data_path):
        raise FileNotFoundError(f"åŸå§‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {original_data_path}")
    
    # åŠ è½½æ•°æ®
    df = pd.read_excel(original_data_path)
    
    # æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    # å¤„ç†ç¼ºå¤±å€¼
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            df[col] = df[col].fillna(df[col].median())
        else:
            df[col] = df[col].fillna(df[col].mode()[0])
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]
    
    # å¤„ç†åˆ†ç±»ç‰¹å¾
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
    
    # å¤„ç†ç›®æ ‡å˜é‡
    if y.dtype == 'object' or y.dtype.name == 'category':
        le = LabelEncoder()
        y = le.fit_transform(y)
    
    # åŠ è½½é¢„è®­ç»ƒçš„scalerï¼ˆå…³é”®ï¼šé¿å…æ•°æ®æ³„éœ²ï¼‰
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"Scaleræ–‡ä»¶ä¸å­˜åœ¨: {scaler_path}")
    
    scaler = joblib.load(scaler_path)
    X_scaled = scaler.transform(X)  # åªtransformï¼Œä¸fit
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )
    
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}, æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test, X.columns.tolist(), is_multiclass

def build_optimize_super_learner(loaded_models, X_train, X_test, y_train, y_test, output_dir, is_multiclass, paths_info):
    """æ„å»ºå’Œä¼˜åŒ–Super Learner"""
    print("\n" + "="*80)
    print("ğŸš€ æ„å»ºå’Œä¼˜åŒ– Super Learner é›†æˆæ¨¡å‹")
    print("="*80)
    
    # è¿‡æ»¤åŸºç¡€æ¨¡å‹ï¼šåªé€‰æ‹©CVå‡†ç¡®ç‡>0.7çš„æ¨¡å‹
    results_dir = f"{output_dir}/results"
    valid_models = {}
    
    for model_name, model in loaded_models.items():
        result_file = f"{results_dir}/{model_name}_results.json"
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                try:
                    result = json.load(f)
                    cv_accuracy = result.get('cv_accuracy', 0)
                    if cv_accuracy > 0.7:  # é˜ˆå€¼å¯è°ƒæ•´
                        valid_models[model_name] = model
                        print(f"âœ… é€‰æ‹©åŸºç¡€æ¨¡å‹: {model_name} (CVå‡†ç¡®ç‡: {cv_accuracy:.4f})")
                    else:
                        print(f"âŒ è·³è¿‡åŸºç¡€æ¨¡å‹: {model_name} (CVå‡†ç¡®ç‡: {cv_accuracy:.4f} < 0.7)")
                except Exception as e:
                    print(f"âš ï¸ è¯»å– {model_name} ç»“æœå¤±è´¥: {str(e)}")
                    valid_models[model_name] = model  # ä¿å®ˆèµ·è§è¿˜æ˜¯åŒ…å«
        else:
            valid_models[model_name] = model  # åŒ…å«æ²¡æœ‰ç»“æœæ–‡ä»¶çš„æ¨¡å‹
    
    if len(valid_models) < 2:
        print(f"âš ï¸ æœ‰æ•ˆåŸºç¡€æ¨¡å‹æ•°é‡ä¸è¶³ ({len(valid_models)} < 2)ï¼Œä½¿ç”¨æ‰€æœ‰åŠ è½½çš„æ¨¡å‹")
        valid_models = loaded_models
    
    # å‡†å¤‡åŸºç¡€æ¨¡å‹åˆ—è¡¨
    estimators = []
    for model_name, model in valid_models.items():
        clean_name = model_name.replace(' ', '_').replace('-', '_').lower()
        estimators.append((clean_name, model))
        print(f"ğŸ”§ æ·»åŠ åŸºç¡€æ¨¡å‹: {model_name} -> {clean_name}")
    
    print(f"\nğŸ¯ å°†ä½¿ç”¨ {len(estimators)} ä¸ªåŸºç¡€æ¨¡å‹æ„å»ºSuper Learner")
    
    # åŠ¨æ€é€‰æ‹©å…ƒæ¨¡å‹
    if is_multiclass:
        meta_learner = RidgeClassifier(random_state=42)
        print("ğŸ“Š å¤šåˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨ RidgeClassifier ä½œä¸ºå…ƒæ¨¡å‹")
    else:
        meta_learner = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear')
        print("ğŸ“Š äºŒåˆ†ç±»é—®é¢˜ï¼Œä½¿ç”¨ LogisticRegression ä½œä¸ºå…ƒæ¨¡å‹")
    
    # å®šä¹‰Super Learnerå‚æ•°ç½‘æ ¼
    param_grid = {
        'stack_method': ['auto', 'predict_proba'] if not is_multiclass else ['auto'],
        'passthrough': [True, False],
        'cv': [5, 10]
    }
    
    # ä¸ºå…ƒæ¨¡å‹æ·»åŠ å‚æ•°
    if is_multiclass:
        param_grid['final_estimator__alpha'] = [0.1, 1.0, 10.0]
    else:
        param_grid['final_estimator__C'] = [0.1, 1.0, 10.0]
        param_grid['final_estimator__class_weight'] = ['balanced', None]
    
    # åˆ›å»ºSuper Learner
    super_learner = StackingClassifier(
        estimators=estimators,
        final_estimator=meta_learner,
        cv=5,
        n_jobs=-1,
        verbose=0
    )
    
    print("\nğŸ”§ å¼€å§‹Super Learnerå‚æ•°ä¼˜åŒ–...")
    
    # 10æŠ˜äº¤å‰éªŒè¯
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    
    # ç½‘æ ¼æœç´¢
    grid_search = GridSearchCV(
        estimator=super_learner,
        param_grid=param_grid,
        cv=cv,
        scoring='accuracy',
        n_jobs=-1,
        verbose=1
    )
    
    start_time = time.time()
    grid_search.fit(X_train, y_train)
    training_time = time.time() - start_time
    
    best_super_learner = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_score = grid_search.best_score_
    
    print(f"\nâœ… Super Learner ä¼˜åŒ–å®Œæˆ!")
    print(f"æœ€ä½³å‚æ•°: {best_params}")
    print(f"æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®ç‡: {best_score:.4f}")
    print(f"è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
    
    # 10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
    scoring = {
        'accuracy': 'accuracy',
        'recall': 'recall_macro' if is_multiclass else 'recall',
        'f1': 'f1_macro' if is_multiclass else 'f1',
        'roc_auc': 'roc_auc_ovr' if is_multiclass else 'roc_auc'
    }
    
    cv_results = cross_validate(
        best_super_learner, X_train, y_train, cv=cv,
        scoring=scoring,
        n_jobs=-1,
        return_train_score=False
    )
    
    # è®¡ç®—95%ç½®ä¿¡åŒºé—´
    def calculate_ci(scores):
        n = len(scores)
        mean = np.mean(scores)
        std = np.std(scores, ddof=1)
        margin = 1.96 * std / np.sqrt(n)
        return mean, mean - margin, mean + margin
    
    cv_metrics = {}
    for metric_name in ['test_accuracy', 'test_recall', 'test_f1']:
        if metric_name in cv_results:
            scores = cv_results[metric_name]
            mean, lower, upper = calculate_ci(scores)
            cv_metrics[metric_name] = {
                'mean': mean,
                'lower_ci': lower,
                'upper_ci': upper,
                'scores': scores.tolist()
            }
    
    # æµ‹è¯•é›†è¯„ä¼°
    y_pred = best_super_learner.predict(X_test)
    test_accuracy = accuracy_score(y_test, y_pred)
    test_recall = recall_score(y_test, y_pred, average='macro' if is_multiclass else 'binary')
    test_f1 = f1_score(y_test, y_pred, average='macro' if is_multiclass else 'binary')
    
    # è®¡ç®—å…¶ä»–æŒ‡æ ‡
    test_specificity = specificity_score(y_test, y_pred, is_multiclass)
    test_ppv = ppv_score(y_test, y_pred, is_multiclass)
    test_npv = npv_score(y_test, y_pred, is_multiclass)
    
    # è®¡ç®—AUC
    test_auc = None
    if hasattr(best_super_learner, 'predict_proba'):
        try:
            y_proba = best_super_learner.predict_proba(X_test)
            if is_multiclass:
                test_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
            else:
                test_auc = roc_auc_score(y_test, y_proba[:, 1])
        except Exception as e:
            print(f"âš ï¸ AUCè®¡ç®—å¤±è´¥: {str(e)}")
            test_auc = None
    
    print(f"\nğŸ“Š Super Learner æµ‹è¯•é›†æ€§èƒ½:")
    print(f"å‡†ç¡®ç‡: {test_accuracy:.4f}")
    print(f"å¬å›ç‡: {test_recall:.4f}")
    print(f"F1åˆ†æ•°: {test_f1:.4f}")
    if test_auc is not None:
        print(f"AUC: {test_auc:.4f}")
    print(f"ç‰¹å¼‚æ€§: {test_specificity:.4f}")
    print(f"é˜³æ€§é¢„æµ‹å€¼: {test_ppv:.4f}")
    print(f"é˜´æ€§é¢„æµ‹å€¼: {test_npv:.4f}")
    
    # ä¿å­˜Super Learner
    super_learner_dir = f"{output_dir}/super_learner"
    os.makedirs(super_learner_dir, exist_ok=True)
    
    model_path = f"{super_learner_dir}/super_learner_best.pkl"
    joblib.dump(best_super_learner, model_path)
    print(f"ğŸ’¾ Super Learner æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    # ä¿å­˜å‚æ•°
    params_path = f"{super_learner_dir}/super_learner_params.json"
    with open(params_path, 'w') as f:
        json.dump(best_params, f, indent=4)
    print(f"ğŸ“ Super Learner å‚æ•°å·²ä¿å­˜: {params_path}")
    
    # åˆ†æå…ƒæ¨¡å‹æƒé‡
    try:
        if hasattr(best_super_learner.final_estimator_, 'coef_'):
            coef = best_super_learner.final_estimator_.coef_[0] if is_multiclass else best_super_learner.final_estimator_.coef_[0]
            base_model_names = [name for name, _ in estimators]
            
            weights_df = pd.DataFrame({
                'base_model': base_model_names,
                'weight': coef
            }).sort_values('weight', ascending=False)
            
            weights_path = f"{super_learner_dir}/meta_model_weights.csv"
            weights_df.to_csv(weights_path, index=False)
            print(f"ğŸ“Š å…ƒæ¨¡å‹æƒé‡å·²ä¿å­˜: {weights_path}")
            
            # ç»˜åˆ¶æƒé‡å›¾
            plt.figure(figsize=(12, 8))
            sns.barplot(x='weight', y='base_model', data=weights_df)
            plt.title('Super Learner - å…ƒæ¨¡å‹æƒé‡åˆ†æ', fontsize=16)
            plt.xlabel('æƒé‡', fontsize=12)
            plt.ylabel('åŸºç¡€æ¨¡å‹', fontsize=12)
            plt.tight_layout()
            plt.savefig(f"{super_learner_dir}/meta_model_weights.png", dpi=300)
            plt.close()
            print(f"ğŸ“ˆ å…ƒæ¨¡å‹æƒé‡å›¾å·²ä¿å­˜: {super_learner_dir}/meta_model_weights.png")
    except Exception as e:
        print(f"âš ï¸ å…ƒæ¨¡å‹æƒé‡åˆ†æå¤±è´¥: {str(e)}")
    
    # ä¿å­˜ç»“æœ
    super_learner_result = {
        'model_name': 'Super_Learner',
        'base_models': [name for name, _ in estimators],
        'best_params': best_params,
        'best_cv_score': best_score,
        'training_time': training_time,
        'cv_metrics': cv_metrics,
        'test_metrics': {
            'accuracy': test_accuracy,
            'recall': test_recall,
            'f1': test_f1,
            'auc': test_auc,
            'specificity': test_specificity,
            'ppv': test_ppv,
            'npv': test_npv
        },
        'model_path': model_path,
        'params_path': params_path
    }
    
    result_path = f"{super_learner_dir}/super_learner_results.json"
    with open(result_path, 'w') as f:
        json.dump(super_learner_result, f, indent=4, default=str)
    print(f"ğŸ“Š Super Learner ç»“æœå·²ä¿å­˜: {result_path}")
    
    # ä¸å•ä¸€æœ€ä½³æ¨¡å‹å¯¹æ¯”
    compare_with_best_single_model(super_learner_result, output_dir, is_multiclass)
    
    return super_learner_result

def compare_with_best_single_model(super_learner_result, output_dir, is_multiclass):
    """ä¸å•ä¸€æœ€ä½³æ¨¡å‹è¿›è¡Œå¯¹æ¯”"""
    print("\n" + "="*80)
    print("ğŸ¯ ä¸å•ä¸€æœ€ä½³æ¨¡å‹å¯¹æ¯”")
    print("="*80)
    
    # åŠ è½½ä¹‹å‰è®­ç»ƒçš„æ¨¡å‹ç»“æœ
    results_dir = f"{output_dir}/results"
    if not os.path.exists(results_dir):
        print("âš ï¸ æ‰¾ä¸åˆ°ä¹‹å‰çš„æ¨¡å‹ç»“æœç›®å½•")
        return
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹ç»“æœ
    model_results = []
    for file in os.listdir(results_dir):
        if file.endswith('_results.json') and file != 'super_learner_results.json':
            file_path = os.path.join(results_dir, file)
            with open(file_path, 'r') as f:
                try:
                    result = json.load(f)
                    if 'test_metrics' in result and 'error' not in result:
                        model_results.append(result)
                except Exception as e:
                    print(f"âš ï¸ è¯»å– {file} å¤±è´¥: {str(e)}")
                    continue
    
    if not model_results:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹ç»“æœ")
        return
    
    # æ‰¾å‡ºå•ä¸€æœ€ä½³æ¨¡å‹
    best_single_model = max(model_results, key=lambda x: x['test_metrics']['accuracy'])
    
    print(f"ğŸ† å•ä¸€æœ€ä½³æ¨¡å‹: {best_single_model['model_name']}")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {best_single_model['test_metrics']['accuracy']:.4f}")
    print(f"   æ¨¡å‹è·¯å¾„: {best_single_model['model_path']}")
    
    print(f"\nğŸ¤– Super Learner:")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {super_learner_result['test_metrics']['accuracy']:.4f}")
    print(f"   æ¨¡å‹è·¯å¾„: {super_learner_result['model_path']}")
    
    # æ€§èƒ½å¯¹æ¯”
    sl_accuracy = super_learner_result['test_metrics']['accuracy']
    single_accuracy = best_single_model['test_metrics']['accuracy']
    
    if sl_accuracy > single_accuracy:
        improvement = (sl_accuracy - single_accuracy) / single_accuracy * 100
        print(f"\nğŸ‰ Super Learner æ¯”å•ä¸€æœ€ä½³æ¨¡å‹è¡¨ç°æ›´å¥½!")
        print(f"   å‡†ç¡®ç‡æå‡: {improvement:.2f}%")
        print(f"   æ¨èä½¿ç”¨ Super Learner é›†æˆæ¨¡å‹")
    else:
        improvement = (single_accuracy - sl_accuracy) / sl_accuracy * 100
        print(f"\nğŸ’¡ å•ä¸€æœ€ä½³æ¨¡å‹è¡¨ç°ç•¥å¥½")
        print(f"   ä¼˜åŠ¿: {improvement:.2f}%")
        print(f"   ä½† Super Learner é€šå¸¸æ›´ç¨³å®šï¼Œå¯æ ¹æ®éœ€æ±‚é€‰æ‹©")
    
    # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    comparison_data = {
        'Model': ['Super_Learner', best_single_model['model_name']],
        'Accuracy': [sl_accuracy, single_accuracy],
        'Recall': [
            super_learner_result['test_metrics']['recall'],
            best_single_model['test_metrics']['recall']
        ],
        'F1_Score': [
            super_learner_result['test_metrics']['f1'],
            best_single_model['test_metrics']['f1']
        ],
        'Type': ['Ensemble', 'Single_Model']
    }
    
    if 'auc' in super_learner_result['test_metrics']:
        comparison_data['AUC'] = [
            super_learner_result['test_metrics']['auc'],
            best_single_model['test_metrics'].get('auc', 0)
        ]
    
    comparison_df = pd.DataFrame(comparison_data)
    
    # ä¿å­˜å¯¹æ¯”ç»“æœ
    comparison_path = f"{output_dir}/super_learner/model_comparison.csv"
    comparison_df.to_csv(comparison_path, index=False)
    print(f"ğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœå·²ä¿å­˜: {comparison_path}")
    
    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Model', y='Accuracy', data=comparison_df, hue='Type')
    plt.title('Super Learner vs æœ€ä½³å•ä¸€æ¨¡å‹ - å‡†ç¡®ç‡å¯¹æ¯”', fontsize=16)
    plt.xlabel('æ¨¡å‹', fontsize=12)
    plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
    plt.tight_layout()
    plt.savefig(f"{output_dir}/super_learner/model_comparison.png", dpi=300)
    plt.close()
    print(f"ğŸ“ˆ æ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜: {output_dir}/super_learner/model_comparison.png")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    model_paths_file = "all_model_paths.json"
    
    print("ğŸš€ å¼€å§‹Super Learneræ„å»ºæµç¨‹")
    print(f"æ¨¡å‹è·¯å¾„æ–‡ä»¶: {model_paths_file}")
    
    # 1. åŠ è½½ä¿å­˜çš„æœ€ä¼˜æ¨¡å‹
    loaded_models, output_dir, n_classes, is_multiclass, scaler_path, paths_info = load_saved_models(model_paths_file)
    
    if not loaded_models:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æ¨¡å‹ï¼Œæµç¨‹ç»ˆæ­¢")
        return
    
    # 2. åŠ è½½åŸå§‹æ•°æ®ï¼ˆå…³é”®ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„scaleré¿å…æ•°æ®æ³„éœ²ï¼‰
    X_train, X_test, y_train, y_test, feature_names, is_multiclass = load_data_for_super_learner(paths_info)
    
    # 3. æ„å»ºå’Œä¼˜åŒ–Super Learner
    super_learner_result = build_optimize_super_learner(
        loaded_models, X_train, X_test, y_train, y_test, output_dir, is_multiclass, paths_info
    )
    
    print("\nğŸ‰ Super Learneræ„å»ºæµç¨‹å®Œæˆ!")
    print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}/super_learner/")

if __name__ == "__main__":
    main()