import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import make_scorer, accuracy_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
import xgboost as xgb
import lightgbm as lgb
import joblib
import warnings
from tqdm import tqdm
import time
import os
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

def calculate_confidence_interval(scores, confidence=0.95):
    """è®¡ç®—95%ç½®ä¿¡åŒºé—´"""
    n = len(scores)
    mean = np.mean(scores)
    std = np.std(scores, ddof=1)
    margin = 1.96 * std / np.sqrt(n)  # 95% CI
    return mean, mean - margin, mean + margin

def specificity_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—ç‰¹å¼‚æ€§ (Specificity)ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        # è®¡ç®—å®å¹³å‡ç‰¹å¼‚æ€§
        cm = confusion_matrix(y_true, y_pred)
        specificities = []
        for i in range(len(cm)):
            tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
            fp = np.sum(cm[:, i]) - cm[i, i]
            spec = tn / (tn + fp) if (tn + fp) > 0 else 0
            specificities.append(spec)
        return np.mean(specificities)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tn / (tn + fp) if (tn + fp) > 0 else 0

def npv_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—é˜´æ€§é¢„æµ‹å€¼ (NPV)ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        # è®¡ç®—å®å¹³å‡NPV
        cm = confusion_matrix(y_true, y_pred)
        npvs = []
        for i in range(len(cm)):
            tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
            fn = np.sum(cm[i, :]) - cm[i, i]
            npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            npvs.append(npv)
        return np.mean(npvs)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tn / (tn + fn) if (tn + fn) > 0 else 0

def ppv_score(y_true, y_pred, is_multiclass=False):
    """è®¡ç®—é˜³æ€§é¢„æµ‹å€¼ (PPV/Precision)ï¼Œæ”¯æŒå¤šåˆ†ç±»"""
    if is_multiclass:
        # è®¡ç®—å®å¹³å‡PPV
        cm = confusion_matrix(y_true, y_pred)
        ppvs = []
        for i in range(len(cm)):
            tp = cm[i, i]
            fp = np.sum(cm[:, i]) - tp
            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
            ppvs.append(ppv)
        return np.mean(ppvs)
    else:
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
        return tp / (tp + fp) if (tp + fp) > 0 else 0

def load_and_preprocess_data(file_path):
    """è¯»å–Excelæ–‡ä»¶å¹¶è¿›è¡Œé¢„å¤„ç†"""
    print("=== è¯»å–Excelæ–‡ä»¶ ===")
    df = pd.read_excel(file_path)
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # å¤„ç†ç¼ºå¤±å€¼
    missing_values = df.isnull().sum()
    if missing_values.sum() > 0:
        print("å­˜åœ¨ç¼ºå¤±å€¼ï¼Œè¿›è¡Œå¡«å……...")
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                df[col] = df[col].fillna(df[col].median())
            else:
                df[col] = df[col].fillna(df[col].mode()[0])
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.iloc[:, :-1]
    y = df.iloc[:, -1]
    
    print(f"ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"ç›®æ ‡å˜é‡å”¯ä¸€å€¼: {y.unique()}")
    
    # å¤„ç†åˆ†ç±»ç‰¹å¾
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        print(f"å‘ç°åˆ†ç±»ç‰¹å¾: {categorical_cols.tolist()}")
        X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)
    
    # å¤„ç†ç›®æ ‡å˜é‡
    is_classification = True
    if y.dtype == 'object' or y.dtype.name == 'category':
        le = LabelEncoder()
        y = le.fit_transform(y)
        print(f"ç¼–ç åçš„ç›®æ ‡å˜é‡: {np.unique(y)}")
    
    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled = pd.DataFrame(X_scaled, columns=X.columns)
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) > 1 else None
    )
    
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}, æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}")
    
    return X_train, X_test, y_train, y_test, X.columns.tolist(), len(np.unique(y)), scaler, file_path

def train_and_evaluate_models(X_train, X_test, y_train, y_test, n_classes, scaler, original_data_path, output_dir):
    """è®­ç»ƒã€ä¼˜åŒ–å’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(f"{output_dir}/models", exist_ok=True)
    os.makedirs(f"{output_dir}/results", exist_ok=True)
    
    is_multiclass = n_classes > 2
    
    # å®šä¹‰è¯„åˆ†å™¨
    scoring = {
        'accuracy': 'accuracy',
        'recall': 'recall_macro' if is_multiclass else 'recall',
        'f1': 'f1_macro' if is_multiclass else 'f1',
        'roc_auc': 'roc_auc_ovr' if is_multiclass else 'roc_auc'
    }
    
    # è‡ªå®šä¹‰è¯„åˆ†å™¨
    custom_scorers = {
        'specificity': make_scorer(lambda y_true, y_pred: specificity_score(y_true, y_pred, is_multiclass)),
        'ppv': make_scorer(lambda y_true, y_pred: ppv_score(y_true, y_pred, is_multiclass)),
        'npv': make_scorer(lambda y_true, y_pred: npv_score(y_true, y_pred, is_multiclass))
    }
    
    # å®šä¹‰æ‰€æœ‰æ¨¡å‹åŠå…¶å‚æ•°ç½‘æ ¼ï¼ˆä¼˜åŒ–å‚æ•°èŒƒå›´ï¼‰
    models_config = {
        'KNN': {
            'model': KNeighborsClassifier(),
            'param_grid': {
                'n_neighbors': [3, 5, 7],
                'weights': ['uniform', 'distance'],
                'metric': ['euclidean', 'manhattan']
            }
        },
        'GNB': {
            'model': GaussianNB(),
            'param_grid': {
                'var_smoothing': [1e-9, 1e-8, 1e-7]
            }
        },
        'LR': {
            'model': LogisticRegression(random_state=42, max_iter=1000),
            'param_grid': {
                'C': [0.01, 0.1, 1, 10],
                'solver': ['lbfgs', 'liblinear'] if not is_multiclass else ['lbfgs'],
                'penalty': ['l2']
            }
        },
        'SVM': {
            'model': SVC(random_state=42, probability=True, cache_size=2000),  # å¢åŠ ç¼“å­˜
            'param_grid': {
                'C': [0.1, 1, 10],
                'kernel': ['rbf', 'linear'],
                'gamma': ['scale', 0.1]
            }
        },
        'DT': {
            'model': DecisionTreeClassifier(random_state=42),
            'param_grid': {
                'max_depth': [None, 5, 10],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2],
                'criterion': ['gini', 'entropy']
            }
        },
        'RF': {
            'model': RandomForestClassifier(random_state=42, n_jobs=-1),
            'param_grid': {
                'n_estimators': [50, 100],
                'max_depth': [None, 10],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            }
        },
        'ET': {
            'model': ExtraTreesClassifier(random_state=42, n_jobs=-1),
            'param_grid': {
                'n_estimators': [50, 100],
                'max_depth': [None, 10],
                'min_samples_split': [2, 5],
                'min_samples_leaf': [1, 2]
            }
        },
        'HGB': {
            'model': HistGradientBoostingClassifier(random_state=42),
            'param_grid': {
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5],
                'min_samples_leaf': [10, 20]
            }
        },
        'XGBoost': {
            'model': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
            'param_grid': {
                'n_estimators': [50, 100],
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5],
                'subsample': [0.8, 1.0]
            }
        },
        'LightGBM': {
            'model': lgb.LGBMClassifier(random_state=42),
            'param_grid': {
                'n_estimators': [50, 100],
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5],
                'num_leaves': [31, 63]
            }
        },
        'MLP': {
            'model': MLPClassifier(random_state=42, max_iter=500, batch_size=128),  # é™åˆ¶è¿­ä»£æ¬¡æ•°å’Œæ‰¹é‡å¤§å°
            'param_grid': {
                'hidden_layer_sizes': [(50,), (100,)],
                'activation': ['relu'],
                'alpha': [0.0001, 0.001],
                'learning_rate': ['constant']
            }
        }
    }
    
    results = {}
    all_model_paths = {}
    
    print("=== å¼€å§‹è®­ç»ƒå’Œä¼˜åŒ–æ‰€æœ‰æ¨¡å‹ ===")
    
    for model_name, config in tqdm(models_config.items(), desc="æ¨¡å‹è®­ç»ƒè¿›åº¦"):
        print(f"\n{'='*70}")
        print(f"ğŸ¯ è®­ç»ƒæ¨¡å‹: {model_name}")
        print(f"{'='*70}")
        
        try:
            # 10æŠ˜äº¤å‰éªŒè¯
            cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
            
            # ç½‘æ ¼æœç´¢
            grid_search = GridSearchCV(
                estimator=config['model'],
                param_grid=config['param_grid'],
                cv=cv,
                scoring='accuracy',
                n_jobs=-1,
                verbose=0  # å‡å°‘è¾“å‡º
            )
            
            start_time = time.time()
            grid_search.fit(X_train, y_train)
            training_time = time.time() - start_time
            
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
            best_score = grid_search.best_score_
            
            print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ!")
            print(f"æœ€ä½³å‚æ•°: {best_params}")
            print(f"æœ€ä½³äº¤å‰éªŒè¯å‡†ç¡®ç‡: {best_score:.4f}")
            print(f"è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")
            
            # è¯¦ç»†çš„10æŠ˜äº¤å‰éªŒè¯è¯„ä¼°
            cv_results = cross_validate(
                best_model, X_train, y_train, cv=cv,
                scoring=scoring,
                n_jobs=-1,
                return_train_score=False
            )
            
            # è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡
            custom_results = {name: [] for name in custom_scorers.keys()}
            for train_idx, test_idx in cv.split(X_train, y_train):
                X_train_fold, X_test_fold = X_train.iloc[train_idx], X_train.iloc[test_idx]
                y_train_fold, y_test_fold = y_train[train_idx], y_train[test_idx]
                
                fold_model = best_model.fit(X_train_fold, y_train_fold)
                y_pred = fold_model.predict(X_test_fold)
                
                for name, scorer in custom_scorers.items():
                    custom_results[name].append(scorer(y_test_fold, y_pred))
            
            # è®¡ç®—å„æŒ‡æ ‡çš„95%ç½®ä¿¡åŒºé—´
            metrics_with_ci = {}
            for metric_name in ['test_accuracy', 'test_recall', 'test_f1']:
                if metric_name in cv_results:
                    scores = cv_results[metric_name]
                    mean, lower, upper = calculate_confidence_interval(scores)
                    metrics_with_ci[metric_name] = {
                        'mean': mean,
                        'lower_ci': lower,
                        'upper_ci': upper,
                        'scores': scores.tolist()
                    }
            
            # AUCéœ€è¦ç‰¹æ®Šå¤„ç†
            if 'test_roc_auc' in cv_results:
                auc_scores = cv_results['test_roc_auc']
                mean_auc, lower_auc, upper_auc = calculate_confidence_interval(auc_scores)
                metrics_with_ci['test_roc_auc'] = {
                    'mean': mean_auc,
                    'lower_ci': lower_auc,
                    'upper_ci': upper_auc,
                    'scores': auc_scores.tolist()
                }
            
            # è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡çš„CI
            for name, scores in custom_results.items():
                if scores:  # ç¡®ä¿æœ‰åˆ†æ•°
                    mean, lower, upper = calculate_confidence_interval(scores)
                    metrics_with_ci[name] = {
                        'mean': mean,
                        'lower_ci': lower,
                        'upper_ci': upper,
                        'scores': scores
                    }
            
            # æµ‹è¯•é›†è¯„ä¼°
            y_pred = best_model.predict(X_test)
            test_accuracy = accuracy_score(y_test, y_pred)
            test_recall = recall_score(y_test, y_pred, average='macro' if is_multiclass else 'binary')
            test_f1 = f1_score(y_test, y_pred, average='macro' if is_multiclass else 'binary')
            
            # è®¡ç®—AUC
            test_auc = None
            if hasattr(best_model, 'predict_proba'):
                try:
                    y_proba = best_model.predict_proba(X_test)
                    if is_multiclass:
                        test_auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
                    else:
                        test_auc = roc_auc_score(y_test, y_proba[:, 1])
                except Exception as e:
                    print(f"âš ï¸ AUCè®¡ç®—å¤±è´¥: {str(e)}")
                    test_auc = None
            
            # è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡
            test_specificity = specificity_score(y_test, y_pred, is_multiclass)
            test_ppv = ppv_score(y_test, y_pred, is_multiclass)
            test_npv = npv_score(y_test, y_pred, is_multiclass)
            
            print(f"\nğŸ“Š {model_name} æµ‹è¯•é›†æ€§èƒ½:")
            print(f"å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"å¬å›ç‡: {test_recall:.4f}")
            print(f"F1åˆ†æ•°: {test_f1:.4f}")
            if test_auc is not None:
                print(f"AUC: {test_auc:.4f}")
            print(f"ç‰¹å¼‚æ€§: {test_specificity:.4f}")
            print(f"é˜³æ€§é¢„æµ‹å€¼: {test_ppv:.4f}")
            print(f"é˜´æ€§é¢„æµ‹å€¼: {test_npv:.4f}")
            
            # ä¿å­˜æ¨¡å‹
            model_path = f"{output_dir}/models/{model_name.replace(' ', '_')}_best.pkl"
            joblib.dump(best_model, model_path)
            print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
            
            # ä¿å­˜å‚æ•°
            params_path = f"{output_dir}/models/{model_name.replace(' ', '_')}_params.json"
            with open(params_path, 'w') as f:
                json.dump(best_params, f, indent=4)
            print(f"ğŸ“ å‚æ•°å·²ä¿å­˜: {params_path}")
            
            # ä¿å­˜ç»“æœ
            model_result = {
                'model_name': model_name,
                'best_params': best_params,
                'best_cv_score': best_score,
                'training_time': training_time,
                'cv_metrics': metrics_with_ci,
                'test_metrics': {
                    'accuracy': test_accuracy,
                    'recall': test_recall,
                    'f1': test_f1,
                    'auc': test_auc,
                    'specificity': test_specificity,
                    'ppv': test_ppv,
                    'npv': test_npv
                },
                'cv_accuracy': best_score,  # ç”¨äºåç»­è¿‡æ»¤
                'model_path': model_path,
                'params_path': params_path
            }
            
            results[model_name] = model_result
            all_model_paths[model_name] = model_path
            
            # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
            result_path = f"{output_dir}/results/{model_name}_results.json"
            with open(result_path, 'w') as f:
                json.dump(model_result, f, indent=4, default=str)
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜: {result_path}")
            
        except Exception as e:
            print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {str(e)}")
            results[model_name] = {'error': str(e)}
    
    # ä¿å­˜scaler
    scaler_path = f"{output_dir}/scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"âœ… Scalerå·²ä¿å­˜: {scaler_path}")
    
    # åˆ›å»ºç»¼åˆç»“æœæŠ¥å‘Š
    create_comprehensive_report(results, output_dir, is_multiclass)
    
    # ä¿å­˜æ‰€æœ‰æ¨¡å‹è·¯å¾„
    paths_info = {
        'model_paths': all_model_paths,
        'output_dir': output_dir,
        'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S'),
        'n_classes': n_classes,
        'is_multiclass': is_multiclass,
        'original_data_path': original_data_path,
        'scaler_path': scaler_path
    }
    
    paths_path = f"{output_dir}/all_model_paths.json"
    with open(paths_path, 'w') as f:
        json.dump(paths_info, f, indent=4)
    print(f"\nâœ… æ‰€æœ‰æ¨¡å‹è·¯å¾„ä¿¡æ¯å·²ä¿å­˜: {paths_path}")
    
    return results, paths_info

def create_comprehensive_report(results, output_dir, is_multiclass):
    """åˆ›å»ºç»¼åˆæŠ¥å‘Š"""
    # å‡†å¤‡æ•°æ®
    report_data = []
    for model_name, result in results.items():
        if 'error' in result:
            report_data.append({
                'Model': model_name,
                'Status': 'Failed',
                'Error': result['error']
            })
            continue
        
        cv_metrics = result['cv_metrics']
        test_metrics = result['test_metrics']
        
        # æå–95% CI
        accuracy_ci = cv_metrics.get('test_accuracy', {})
        recall_ci = cv_metrics.get('test_recall', {})
        f1_ci = cv_metrics.get('test_f1', {})
        auc_ci = cv_metrics.get('test_roc_auc', {})
        
        row = {
            'Model': model_name,
            'Status': 'Success',
            'CV_Accuracy_Mean': accuracy_ci.get('mean', 0),
            'CV_Accuracy_95%CI': f"{accuracy_ci.get('lower_ci', 0):.4f} - {accuracy_ci.get('upper_ci', 0):.4f}",
            'CV_Recall_Mean': recall_ci.get('mean', 0),
            'CV_Recall_95%CI': f"{recall_ci.get('lower_ci', 0):.4f} - {recall_ci.get('upper_ci', 0):.4f}",
            'CV_F1_Mean': f1_ci.get('mean', 0),
            'CV_F1_95%CI': f"{f1_ci.get('lower_ci', 0):.4f} - {f1_ci.get('upper_ci', 0):.4f}",
            'Test_Accuracy': test_metrics['accuracy'],
            'Test_Recall': test_metrics['recall'],
            'Test_F1': test_metrics['f1'],
            'Test_Specificity': test_metrics['specificity'],
            'Test_PPV': test_metrics['ppv'],
            'Test_NPV': test_metrics['npv'],
            'Training_Time(s)': result['training_time']
        }
        
        if 'test_roc_auc' in cv_metrics:
            row['CV_AUC_Mean'] = auc_ci.get('mean', 0)
            row['CV_AUC_95%CI'] = f"{auc_ci.get('lower_ci', 0):.4f} - {auc_ci.get('upper_ci', 0):.4f}"
            row['Test_AUC'] = test_metrics['auc']
        
        report_data.append(row)
    
    # åˆ›å»ºDataFrame
    report_df = pd.DataFrame(report_data)
    
    # ä¿å­˜CSVæŠ¥å‘Š
    csv_path = f"{output_dir}/results/comprehensive_report.csv"
    report_df.to_csv(csv_path, index=False)
    print(f"âœ… ç»¼åˆæŠ¥å‘ŠCSVå·²ä¿å­˜: {csv_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    if 'Test_Accuracy' in report_df.columns:
        plt.figure(figsize=(15, 8))
        successful_models = report_df[report_df['Status'] == 'Success']
        if not successful_models.empty:
            sns.barplot(x='Model', y='Test_Accuracy', data=successful_models)
            plt.title('å„æ¨¡å‹æµ‹è¯•é›†å‡†ç¡®ç‡å¯¹æ¯”', fontsize=16)
            plt.xlabel('æ¨¡å‹', fontsize=12)
            plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f"{output_dir}/results/model_accuracy_comparison.png", dpi=300)
            plt.close()
            print(f"ğŸ“ˆ å‡†ç¡®ç‡å¯¹æ¯”å›¾å·²ä¿å­˜: {output_dir}/results/model_accuracy_comparison.png")

def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®
    excel_file_path = "data.xlsx"
    output_dir = f"model_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒæµç¨‹")
    print(f"Excelæ–‡ä»¶: {excel_file_path}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # 1. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    X_train, X_test, y_train, y_test, feature_names, n_classes, scaler, original_data_path = load_and_preprocess_data(excel_file_path)
    
    # 2. ä¿å­˜é¢„å¤„ç†ä¿¡æ¯
    preprocessing_info = {
        'feature_names': feature_names,
        'n_classes': n_classes,
        'is_multiclass': n_classes > 2,
        'train_shape': X_train.shape,
        'test_shape': X_test.shape,
        'class_distribution': pd.Series(y_train).value_counts().to_dict(),
        'original_data_path': original_data_path
    }
    
    os.makedirs(output_dir, exist_ok=True)
    with open(f"{output_dir}/preprocessing_info.json", 'w') as f:
        json.dump(preprocessing_info, f, indent=4)
    print(f"âœ… é¢„å¤„ç†ä¿¡æ¯å·²ä¿å­˜: {output_dir}/preprocessing_info.json")
    
    # 3. è®­ç»ƒå’Œè¯„ä¼°æ‰€æœ‰æ¨¡å‹
    results, paths_info = train_and_evaluate_models(X_train, X_test, y_train, y_test, n_classes, scaler, original_data_path, output_dir)
    
    print("\nğŸ‰ æ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ!")
    print(f"æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"æœ€ä¼˜æ¨¡å‹è·¯å¾„ä¿¡æ¯: {output_dir}/all_model_paths.json")

if __name__ == "__main__":
    main()